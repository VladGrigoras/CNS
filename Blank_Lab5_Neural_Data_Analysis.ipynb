{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VladGrigoras/CNS/blob/main/Blank_Lab5_Neural_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo_lKy7lrfQ0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**If you are on Google Colab, before running anything, you may wish to connect to a GPU environment**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu9CqeQFb2zP"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "In this lab, you will get hands-on experience with neural data. \n",
        "\n",
        "The dataset consists of recordings of the parietal cortex of a monkey during a sensory-motor task. It is one of the datasets from the [Neural Latent Benchmark](https://neurallatents.github.io/) project, whose aim is to provide standardized freely accessible neural datasets for the sake of developing data analysis and modeling methods.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The monkey starts a *trial* of the experiment holding a manipulandum (i.e. a joystick). The manipulandum, and therefore the hand of the animal, is then \"bumped\". The task of the animal is to return the manipulandom to its original central position. The bump can be directed toward one of 8 angles ($0^\\circ,45^\\circ...,315^\\circ$). This is repeated for hundreds of trials. Throughout, the position of the hand of the animal is recorded.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Jyt8sjdagyBY1_kl-rTJhTFHlvdmFRLM)\n",
        "\n",
        "Neurons from area A2 are also recorded using a [microelectrode array](https://www.brainlatam.com/uploads/produto/utah-array-335.webp). Area A2 is located in the somatosensory cortex (parietal cortex) which carries computations related to tactile senses (e.g. feeling the pressure on your fingers when you grab an object). More recent work has suggested that its computations are not purely limited to tactile sense [1], and that, among other things, kinematics (e.g. movement of the arm) is also represented in the neural activity of area A2. \n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=14ZWPqYTF5L33umDcmDCWoWo0WwUV_GAo)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Today we will explore this hypothesis from a phenomenological standpoint by repurposing to this sensory area some of the models classically used in the motor literature. In particular, when it comes to the modeling or data analysis of neural activity related to arm movement, linear dynamical systems are extensively used. The mathematical theory will build upon what was introduced in lab 0, except that you will now *fit* those linear dynamical systems to neural data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhA_2V8ZkTav"
      },
      "source": [
        "#Data imports\n",
        "\n",
        "The neural data is presented in the Neurodata Without Border (NWB) format, which is a classically used format for storing and sharing neural data. Here all the code was written for you, you may just run the following cells which will download the part of the dataset from this experiment which is relevant to this lab: one recording session. Note that this can take a while (<5min) to run. So you should avoid restarting your runtime environment or rerunning those cells. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-khOJ_4KHdhn"
      },
      "source": [
        "### Run this:\n",
        "\n",
        "(no need to expand if you don't want to clutter your ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM6nW80Mvf7u"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# global defaults for plots - optional\n",
        "sns.set_theme(style=\"ticks\",\n",
        "              palette=\"Set2\",\n",
        "              font_scale=1.0,\n",
        "              rc={\n",
        "              \"axes.spines.right\": False,\n",
        "              \"axes.spines.top\": False,\n",
        "          },\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00nl4ELuKj1t"
      },
      "outputs": [],
      "source": [
        "# You should not evaluate this cell multiple times\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Download the data\n",
        "!pip install git+https://github.com/neurallatents/nlb_tools.git\n",
        "!pip install dandi\n",
        "!dandi download https://gui.dandiarchive.org/dandiset/000127\n",
        "\n",
        "#Install Jax optimization library\n",
        "!pip install optax\n",
        "\n",
        "#Install behavior decoding library\n",
        "!pip install git+https://github.com/arthur-pe/hand-decoding.git#egg=hand_decoding\n",
        "\n",
        "clear_output() #clear cell output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcHq3W1qFt-8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import itertools\n",
        "import scipy\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "import hand_decoding\n",
        "\n",
        "from nlb_tools.nwb_interface import NWBDataset\n",
        "\n",
        "np.random.seed(7)\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JClxUMAKoCr"
      },
      "outputs": [],
      "source": [
        "#The part of the dataset of interest\n",
        "dataset = NWBDataset(\"./000127/sub-Han/\", \"*train\", split_heldout=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmxrx0frKvP1"
      },
      "outputs": [],
      "source": [
        "#Function to import the dataset -- note that we do no use the smoothing argument\n",
        "def get_data(smoothing=70):\n",
        "  dataset.smooth_spk(smoothing, name='smth_40')\n",
        "  continuous_task_variables = ['force', 'hand_pos', 'hand_vel', 'joint_ang', \n",
        "                              'joint_vel', 'muscle_len', 'muscle_vel', 'spikes', \n",
        "                              'spikes_smth_40', 'condition', 'direction']\n",
        "  all_task_variables = continuous_task_variables\n",
        "  task_variables_dict = {key: [] for key in all_task_variables}\n",
        "\n",
        "  for id, cond in enumerate(unique_conditions):\n",
        "      cond_mask = (np.all(dataset.trial_info[['ctr_hold_bump', 'cond_dir']] == cond, axis=1)) & \\\n",
        "                  (dataset.trial_info.split != 'none')\n",
        "      cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-100, 500), ignored_trials=~cond_mask)\n",
        "\n",
        "      for idx, trial in cond_data.groupby('trial_id'):\n",
        "        for var in all_task_variables[:-2]:\n",
        "          cond_tensor = trial[var]\n",
        "          task_variables_dict[var].append(cond_tensor)\n",
        "        task_variables_dict['condition'].append(cond[0])\n",
        "        task_variables_dict['direction'].append(cond[1])\n",
        "\n",
        "  for var in all_task_variables:\n",
        "    task_variables_dict[var] = np.stack(task_variables_dict[var])\n",
        "\n",
        "  return task_variables_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygvPEzOnKxmr"
      },
      "outputs": [],
      "source": [
        "bump = [True] #[False] for voluntary movements, [False, True] for both\n",
        "angle = [i*45.0 for i in range(8)]\n",
        "unique_conditions = list(itertools.product(bump, angle)) #cartesian product\n",
        "\n",
        "task_variables_dict = get_data(smoothing=40) #takes a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSYTVBhT4u0T"
      },
      "outputs": [],
      "source": [
        "#rebinning\n",
        "bin_size = 10\n",
        "neural_data = np.stack([task_variables_dict['spikes'][:,i:i+bin_size].sum(axis=1) \n",
        "      for i in range(0,task_variables_dict['spikes'].shape[1],bin_size)],axis=1)\n",
        "hand_movement = np.stack([task_variables_dict['hand_pos'][:,i:i+bin_size].mean(axis=1) \n",
        "      for i in range(0,task_variables_dict['hand_pos'].shape[1],bin_size)],axis=1)\n",
        "\n",
        "#Task variables\n",
        "angle = task_variables_dict['direction']\n",
        "bump = task_variables_dict['condition']\n",
        "\n",
        "trial_dimension, time_dimension, neuron_dimension = neural_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMxjFs2cmo0H"
      },
      "source": [
        "### Data format\n",
        "\n",
        "The data thus imported are numpy arrays of shape:\n",
        "\n",
        "\n",
        "```\n",
        "neural_data (trial, time, neuron) : (364, 600, 65)\n",
        "hand_movement (trial, time, position) : (364, 600, 2)\n",
        "angle (trial) : (364)\n",
        "```\n",
        "\n",
        "*   `neural_data` contains binned spikes: that is for a given trial and neuron, an array of the form [0,2,0,0,1,...] indicating the number of spikes that occured between 0-10ms, 10-20ms, ... The bump happens at 100ms, that is bin number 10.\n",
        "\n",
        "* `hand_movement` contains the x-y position of the hand of the animal (or more precisely the manipulandum's position), for each trial and over time.\n",
        "\n",
        "*   `angle` contains the angle of the bump (in degrees) for each trial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXjoeNrXlWR5"
      },
      "source": [
        "#Simple statistics\n",
        "\n",
        "A common first step when analyzing a new piece of data is to do basic statistics and plotting to get a sense of the data: \"are there near-silent neurons?\", \"is the behavior stereotypic?\", \"are trials of similar durations?\" are typical questions which might be relevant to answer before starting applying complex models or data analysis methods to the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_hVAJmmmT_K"
      },
      "source": [
        "## Analysis of behavior\n",
        "\n",
        "First, we plot the hand movement over trials, colored by the angle of the bump. As expected, the hand starts at $(x,y)=(0,0)$, is deflected toward the direction of the bump, and the animal pulls it back to $(0,0)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0MhU98hmh8k"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(4,4), constrained_layout=True)\n",
        "ax = fig.add_subplot()\n",
        "\n",
        "cmap = matplotlib.cm.get_cmap('gist_rainbow')\n",
        "\n",
        "for id, pos in enumerate(hand_movement):\n",
        "  ax.plot(pos[:,0], pos[:,1], c=cmap(angle[id]/360), alpha=0.6, linewidth=1)\n",
        "\n",
        "ax.set_xlabel('x (cm)')\n",
        "ax.set_ylabel('y (cm)')\n",
        "ax.set_title('Hand movement')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we plot the average (over trials) velocity of the hand for each bump direction. To not clutter the plot, we only plot the average velocity for 3 bump directions. The shade represents 1 standard deviation."
      ],
      "metadata": {
        "id": "lWiB0VoTKgtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(4,4), constrained_layout=True)\n",
        "ax = fig.add_subplot()\n",
        "\n",
        "cmap = matplotlib.cm.get_cmap('gist_rainbow')\n",
        "\n",
        "ts = np.arange(-10, time_dimension-10)*10\n",
        "\n",
        "for a in np.unique(angle)[-3:]:\n",
        "  hand_movement_of_angle = hand_movement[angle==a]\n",
        "  velocity = np.linalg.norm(hand_movement_of_angle, axis=-1)\n",
        "  ax.plot(ts, velocity.mean(axis=0),\n",
        "          c=cmap(a/360), alpha=0.6, linewidth=1, label='$'+str(a)+'^\\circ$')\n",
        "  \n",
        "  std_velocity = velocity.std(axis=0)#/len(velocity)\n",
        "  ax.fill_between(ts, velocity.mean(axis=0)-std_velocity, velocity.mean(axis=0)+std_velocity,\n",
        "          color=cmap(a/360), alpha=0.1)\n",
        "\n",
        "ax.axvline(0, color='red', label='bump', linestyle='--')\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "ax.set_xlabel('time (ms)')\n",
        "ax.set_ylabel('velocity (cm/ms)')\n",
        "ax.set_title('Average hand velocity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J1eLn-L3LWkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  The bump seems to have less effect when directed toward the animal (dark blue). In particular, the animal returns to the central position faster as represented by the velocity returning to near zero at an earlier time. How may that affect the analysis of the neural data?"
      ],
      "metadata": {
        "id": "EigyrS7jO2ne"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yA8Y7MLUWH3"
      },
      "source": [
        "## Analysis of the neural data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lESeuP4FAwI5"
      },
      "source": [
        "### Neuron-wise average\n",
        "\n",
        "We plot a histogram of the average number of spikes per trial for all neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dClKQln14DtP"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(4,4), constrained_layout=True)\n",
        "ax = fig.add_subplot()\n",
        "\n",
        "ax.hist(neural_data.sum(axis=1).mean(axis=0), bins=20)\n",
        "\n",
        "ax.set_xlabel('average number of spikes'), ax.set_ylabel('number of neurons')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUcFmXkg5xth"
      },
      "source": [
        "*  What do you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXxNUD5p-Tup"
      },
      "source": [
        "### Exercise 1 $-$ Time-wise average\n",
        "\n",
        "Plot the average (over neurons and trials) number of spikes as a function of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWORE6cx-fZP"
      },
      "outputs": [],
      "source": [
        "#Your solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0yW1vT_rGC"
      },
      "source": [
        "*  What do you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning\n",
        "\n",
        "Next we will explore whether neurons are tuned to bump directions. To this end, we will sort the the trials according to reach direction, and average over the time period from -50ms to 200ms the activity of three example neurons."
      ],
      "metadata": {
        "id": "rP4i-7ECQHIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Index_of_neurons_plotted = [0, 1, 2]\n",
        "\n",
        "fig, axes = plt.subplots(1, len(Index_of_neurons_plotted), \n",
        "              figsize=(3*len(Index_of_neurons_plotted),3), constrained_layout=True)\n",
        "\n",
        "cmap = matplotlib.cm.get_cmap('gist_rainbow')\n",
        "\n",
        "unique_angle = np.unique(angle)\n",
        "\n",
        "sorted_id = np.argsort(angle)\n",
        "sorted_angle = angle[sorted_id]\n",
        "\n",
        "for i in Index_of_neurons_plotted:\n",
        "  for a in unique_angle:\n",
        "    axes[i].scatter(a, neural_data[angle==a,5:30].sum(axis=1).mean(axis=0)[i], color=cmap(a/360))\n",
        "  axes[i].set_xlabel('angle (degree)')\n",
        "\n",
        "axes[0].set_ylabel('average number of spikes\\n per trial')\n",
        "  \n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xJVWAqfqQh-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Qualitatively comment on the tuning of these three neurons (you may want to look at other neurons).\n",
        "\n",
        "As it turns out, while the activity of single neurons is poorly tuned to bump direction, the activity of the population of neurons as a whole is highly stereotypical for a given bump direction. We will therefore build a model of the population rather than of single neurons. But before using this model we need to verify that it is a valid one based on simpler statistics."
      ],
      "metadata": {
        "id": "lsSFZeUXUexa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frOOOtgO57Ej"
      },
      "source": [
        "### Exercise 2 $-$ Fano factor\n",
        "\n",
        "As a reminder, the Fano factor is the ratio of the variance to the mean of the data. \n",
        "\n",
        "$$\n",
        "F = \\frac{\\sigma^2}{\\mu}\n",
        "$$\n",
        "\n",
        "We will compute the Fano factor in two ways. First for each neuron, then for each time point.\n",
        "\n",
        "*  Compute the Fano factor over all trials and time for each neuron separately, and plot a histogram of the obtained Fano factors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRLR_2Mq7Rq3"
      },
      "outputs": [],
      "source": [
        "#Your solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRvpEzn58ISb"
      },
      "source": [
        "### Running Fano factor\n",
        "\n",
        "We will next compute the running Fano factor over the whole dataset. That is the Fano factor computed over a given time window $t$ to $t+\\Delta t$ms (here pick a $30$ms $=3$bins window), for all $t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFGyF9Vj8DdV"
      },
      "outputs": [],
      "source": [
        "#Running Fano factor\n",
        "window_size = 3\n",
        "\n",
        "running_fano_factor = []\n",
        "for bin in range(time_dimension-window_size):\n",
        "  mean = neural_data[:,bin:bin+window_size].mean()\n",
        "  var = neural_data[:,bin:bin+window_size].var()\n",
        "  running_fano_factor.append(var/mean)\n",
        "\n",
        "fig = plt.figure(figsize=(4,4), constrained_layout=True)\n",
        "ax = fig.add_subplot()\n",
        "\n",
        "ax.axvline(0, label='bump', color='red', linestyle='--')\n",
        "ax.legend()\n",
        "ax.plot(np.arange(-10, time_dimension-window_size-10)*bin_size, running_fano_factor)\n",
        "\n",
        "ax.set_xlabel('time (ms)'), ax.set_ylabel('running Fano factor')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VifATWba-IBe"
      },
      "source": [
        "*  Given what you have seen in class, what stochastic process may or may not be suited to model this data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5FsF6ggCv6y"
      },
      "source": [
        "# Latent linear dynamical system model\n",
        "\n",
        "Next, we will fit a linear dynamical system to the neural data [2]. Rather than fitting a dynamical system of the dimension of the number of neurons, we will try to find *latent* variables in the neurons recorded. The word *latent* is used in two ways in neuroscience:\n",
        "\n",
        "*  To describe a small set of variables whose (linear) combination describes well the statistics of the population of neurons. \n",
        "*  To describe unobserved variables in probabilistic models. \n",
        "\n",
        "Both interpretations are combined in the model we will use here. First from the fact that our LDS is low dimensional. Second from the fact that we will model spikes $-$ which is required since the data are spikes $-$ using an (inhomogenous) Poisson process whose rate is a linear combination of the variables of the LDS. \n",
        "\n",
        "These latent variables will provide hints as to the computations that may be happening in the neural population recorded. For example, they will turn out to be correlated with the animal's behavior. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For any given trial $i$, and a latent variable $\\mathbf{x}^{(i)}(t)\\in\\mathbb{R}^m$ the dynamical system will be evaluated as,\n",
        "\n",
        "\\begin{aligned}\n",
        "\\frac{d}{dt}\\mathbf{x}^{(i)}=W\\mathbf{x}^{(i)}, &&\\mathbf{x}^{(i)}(0)=\\mathbf{x}_0^{(i)}.\n",
        "\\end{aligned}\n",
        "\n",
        "That is, the *weight matrix* $W$ of the system is shared across all trials, but the initial condition is allowed to vary. We will furthermore constrain the initial condition to be the linear combination of *basis initial conditions*,\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{x}_0^{(i)} = \\sum_{i=j}^r U_{ij}\\mathbf{v}_j.\n",
        "\\end{aligned}\n",
        "\n",
        "where $U\\in\\mathbb{R}^{K\\times r}$ where $k$ is the number of trials and $r$ the number of basis initial conditions. Here, as a guess, we choose $r=8$ since there are $8$ bump directions and the neural activity should be relatively stereotypic for a given bump direction. In practice, one would find the optimal value of $r$ and $m$ through [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics).\n",
        "\n",
        "As you've seen in first week of lab, this system has solution,\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{x}^{(i)}(t) = e^{tW}\\mathbf{x}_0^{(i)}=\\sum_{j=1}^r u_j^{(i)}e^{tW}\\mathbf{v}_j.\n",
        "\\end{aligned}\n",
        "\n",
        "and thus the activity of neurons during a given trial can be expressed as the linear combination of $r$ bases activities $e^{tW}\\mathbf{v}_j$. \n",
        "\n",
        "We ought to consider $\\mathbf{x}^{(i)}(t)\\in\\mathbb{R}^{m}$ (where $m < n$) as the *latent activity* of the population of neuron recorded. We therefore need to introduce a map $M\\in \\mathbb{R}^{n \\times m}$ from this state to the full neural state,\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{\\hat y}^{(i)}(t)=\\exp(M\\mathbf{x}^{(i)}(t))\n",
        "\\end{aligned}\n",
        "\n",
        "where $\\mathbf{\\hat y}^{(i)}(t)$ is the estimate of the firing rate of the neurons recorded at time $t$ during trial $i$. The element-wise exponential is there to ensure that the firing rates are positive.\n",
        "\n",
        "---\n",
        "\n",
        "The variable $\\mathbf{\\hat y}$ is continuous, but we only have access to neurons' spikes. We can however maximize the likelihood of the observed spikes given the estimated firing rate. For this, we can use the results of the previous section and notice that a Poisson process is a decent model for the spikes. It will be an inhomogeneous Poisson process, whose parameter is $\\mathbf{\\hat y}$. Such a model is called a PoissonLDS (or PLDS). \n",
        "\n",
        "Since for a Poisson process the number of spikes in any time interval is independent of the number of spikes in other disjoint time intervals, we can just take the product of the probability to obtain the likelihood of the whole dataset given the estimated rates,\n",
        "\n",
        "\n",
        "\\begin{aligned}\n",
        "p(n | \\mathbf{\\hat y}):&=\\prod_{i=1}^{K}\\prod_{j=1}^{T}\\prod_{k=1}^{N} p(n^{(i)}_{jk}|\\mathbf{\\hat y}^{(i)}_{jk})\n",
        "\\\\&=\\prod_{i=1}^{K}\\prod_{j=1}^{T}\\prod_{j=1}^{N} e^{\\mathbf{\\hat y}^{(i)}_{jk}} \\frac{(\\mathbf{\\hat y}^{(i)}_{jk})^{n^{(i)}_{jk}}}{n^{(i)}_{jk}} \n",
        "\\end{aligned}\n",
        "\n",
        "where $n^{(i)}_{jk}$ is the number of spikes observed in the $j$th bin of the $i$th trial for neuron number $k$, and $\\mathbf{\\hat y}^{(i)}_{jk}$ the corresponding estimated rate (we might for example take the mid time point of the bin). It is difficult to optimize directly on probabilities. In particular since the $p(n^{(i)}_{jk}|\\mathbf{\\hat y}^{(i)}_{jk})$'s are in $[0,1]$, their product is minuscule. We instead take the minus the log of this probability and divide by the number of trials and time points, \n",
        "\n",
        "\\begin{aligned}\n",
        "L(W,\\mathbf{v}_j, \\mathbf{u}^{(i)}, M) = \\frac{1}{KT}\\sum_{i=1}^{K}\\sum_{j=1}^{T}\\sum_{k=1}^{N} -\\log \\left( e^{\\mathbf{\\hat y}^{(i)}_{jk}} \\frac{(\\mathbf{\\hat y}^{(i)}_{jk})^{n^{(i)}_{jk}}}{n^{(i)}_{jk}} \\right)\n",
        "\\end{aligned}\n",
        "\n",
        "Since the negative log function is strictly decreasing, maximizing $p(n | \\mathbf{\\hat y})$ is equivalent to minimizing $L$. To perform this minimization we can use various algorithms. The simplest one is gradient descent which can be implemented easily with machine learning libraries such as Torch, Jax or Tensorflow. \n",
        "\n",
        "---\n",
        "\n",
        "This model might at first seem complex, but its elements allow extracting what is needed to better understand the data. First, it allows going from an arbitrarily large neural population to a small set of *latent* variables which explains it well. Second, it allows going from a large number of trials to a small set of basis trials which describes well all trials. And finally it allows switching from discrete variables (spikes) to continuous ones. These features allow exploring different aspects of the data as we will see bellow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnRpwk3gC34m"
      },
      "source": [
        "**Optional asides.**  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*Tensors*\n",
        "\n",
        "\n",
        "The tensor $[e^{Wt}\\mathbf{v}_1, e^{Wt}\\mathbf{v}_2, ...]\\in \\mathbb{R}^{r \\times m \\times T}$ is the core tensor of a [tucker decomposition](https://en.wikipedia.org/wiki/Tucker_decomposition) of the estimated firing rate before applying the exponential. The matrices $M$ and $U$ are maps,\n",
        "\n",
        "$$\n",
        "\\mathbb{R}^{r \\times m \\times T}\\xrightarrow{M}\\mathbb{R}^{r \\times N \\times T}\\xrightarrow{U}\\mathbb{R}^{K \\times N \\times T}\\xrightarrow{\\text{exp}}\\mathbb{R}^{K \\times N \\times T}_{+},\n",
        "$$\n",
        "\n",
        "Or informally,\n",
        "\n",
        "$$\n",
        "\\\\e^{Wt}\\mathbf{v}_j\\xrightarrow{M}Me^{Wt}\\mathbf{v}_j\\xrightarrow{U,\\text{exp}}\\mathbf{y}^{(i)}_{j},\n",
        "$$\n",
        "\n",
        "which commute by multilinearity,\n",
        "\n",
        "$$\n",
        "\\mathbb{R}^{r \\times m \\times T}\\xrightarrow{U}\\mathbb{R}^{K \\times m \\times T}\\xrightarrow{M}\\mathbb{R}^{K \\times N \\times T}\\xrightarrow{\\text{exp}}\\mathbb{R}^{K \\times N \\times T}_{+}.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\\\e^{Wt}\\mathbf{v}_j\\xrightarrow{U}\\mathbf{x}^{(i)}_j\\xrightarrow{M,\\text{exp}}\\mathbf{y}^{(i)}_j,\n",
        "$$\n",
        "\n",
        "This implies that we can apply either $M$ and $U$ to get latent neural activity $\\mathbf{x}^{(i)}(t)$ or *latent trial activity* $Me^{Wt}\\mathbf{v}(t)$. Low dimensional latent neuron activity usually better describes sensory areas while low dimensional trial activity usually better describes motor areas [3]. This model which allows flexibly varying the dimension of the two is therefore particularly well suited for this sensory-motor dataset.\n",
        "\n",
        "---\n",
        "*Gaussian observations*\n",
        "\n",
        "\n",
        "The model considered above more usually contains a *Gaussian observation* such that $\\ln (\\mathbf{y}^{(i)}(t))\\sim \\mathcal{N}(M\\mathbf{x}^{(i)}(t), \\boldsymbol{\\Sigma})$ (see [2]). This step requires using the [expectation-minimization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) or other variational methods to fit the model to data. However, these algorithms can be computationally expensive and can make it more difficult for the model to converge. We therefore do not consider this case here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypUAAYCPnq3v"
      },
      "source": [
        "## Fitting \n",
        "\n",
        "To fit the model to neural data we use [Jax](https://jax.readthedocs.io/), which is backend for numpy which allows using numpy as-is while being able to compute gradients of operations done on numpy array. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt6pFzLnJGrk"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def lds(params):\n",
        "\n",
        "  Wt = jnp.einsum('i,jk->ijk', ts, params['W']) #W*t, time x latent x latent\n",
        "\n",
        "  Wt_exp = jax.vmap(jax.scipy.linalg.expm)(Wt) #e^(W*t), time x latent x latent\n",
        "\n",
        "  #ue^(W*t)v, trial x time x latent\n",
        "  x = jnp.einsum('ij,jl,mlo->imo', params['U'], params['V'], Wt_exp) \n",
        "\n",
        "  return x\n",
        "\n",
        "def poisson_log_likelihood(rates, spikes):\n",
        "  ratess = rates/100 #spikes are in 10ms bins, rates is in spikes/second\n",
        "  likelihood = jnp.exp(-rates)*jnp.power(rates,spikes)/scipy.special.factorial(spikes)\n",
        "  return jnp.log(likelihood)\n",
        "\n",
        "def L(params, y):\n",
        "\n",
        "      npll = -poisson_log_likelihood(jax.nn.softplus(lds(params) @ params['M'].T), y)\n",
        "\n",
        "      mean_loss = npll[np.random.rand(trial_dimension)<0.5].mean()\n",
        "\n",
        "      return mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJL_Tr1zeuzt"
      },
      "outputs": [],
      "source": [
        "def poisson_LDS_training(y, params, ts, opt_fn, opt_state, steps=100):\n",
        "\n",
        "    losses = []\n",
        "    for step in range(steps):\n",
        "        loss, grads = jax.value_and_grad(L)(params, y)\n",
        "        updates, opt_state = opt_fn(grads, opt_state, params)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "        losses.append(loss)\n",
        "\n",
        "        if step%100==0:\n",
        "          print('SGD step:', step, ' loss:', loss)\n",
        "\n",
        "    return jnp.stack(losses), params, np.array(lds(params)), opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYpykJi3fzrT"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "m = 30\n",
        "r = 8\n",
        "\n",
        "#We discard -100ms to 100ms as there is a lag in neural activity\n",
        "new_time_dimension = time_dimension - 20 \n",
        "y = neural_data[:,20:]\n",
        "\n",
        "#Duration of evaluation of the LDS\n",
        "ts = jnp.linspace(0,new_time_dimension/10,new_time_dimension)\n",
        "\n",
        "#Jax requires a key (seed) for every random variable\n",
        "rng = random.PRNGKey(0)\n",
        "keys = random.split(rng, 3)\n",
        "\n",
        "#Optimized parameters\n",
        "W = random.normal(keys[0], (m, m))/np.sqrt(m) - jnp.eye(m)*3/np.sqrt(m)\n",
        "U = random.normal(keys[0], (trial_dimension, r))/np.sqrt(r)\n",
        "V = random.normal(keys[2], (r, m))/np.sqrt(m)\n",
        "M = random.normal(keys[2], (neuron_dimension, m))/np.sqrt(m)\n",
        "\n",
        "params = {'W' : W, 'U' : U, 'V' : V, 'M' : M}\n",
        "\n",
        "adam = optax.adamw(learning_rate=0.01)\n",
        "\n",
        "losses, params, x, _ = poisson_LDS_training(y, params, ts, steps=3000, \n",
        "                          opt_fn=adam.update, opt_state=adam.init(params))\n",
        "\n",
        "clear_output()\n",
        "fig = plt.figure(figsize=(4,4))\n",
        "ax = fig.add_subplot()\n",
        "ax.plot(losses)\n",
        "ax.set_title('Loss')\n",
        "ax.set_xlabel('iteration')\n",
        "ax.set_ylabel('Poisson negative log likelihood')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUI9NqAQf5Z8"
      },
      "source": [
        "## Hand decoding\n",
        "\n",
        "A really exciting aspect of capturing neural activity with latent variables is that these latent variables are often correlated with behavior. Here, we illustrate this by linearly mapping the latent activity onto hand movement. That is to find the matrix $A\\in \\mathbb{R}^{m \\times 2}$ such that,\n",
        "\n",
        "$$\n",
        "\\sum_i^K ||\\mathbf{x}^{(i)}A - h^{(i)}||^2\n",
        "$$\n",
        "\n",
        "is minimized, where $h^{(i)}\\in \\mathbb{R}^{T\\times 2}$ is the hand position over time at trial $i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GygCDe78iDSW"
      },
      "outputs": [],
      "source": [
        "#note: the hand-decoding library takes time x trial x neuron tensors\n",
        "#you may interchange velocity_decoding with position_decoding\n",
        "decoded_hand_pos = hand_decoding.velocity_decoding(x.transpose(1,0,2), \n",
        "    hand_movement[:,10:-10].transpose(1,0,2), regularization=1)[0].transpose(1,0,2)\n",
        "\n",
        "\n",
        "print('R^2:', hand_decoding.trial_wise_r2(hand_movement[:,10:-10].transpose(1,0,2), \n",
        "                                          decoded_hand_pos.transpose(1,0,2))[0])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8,4), tight_layout=True)\n",
        "\n",
        "for id, pos in enumerate(decoded_hand_pos):\n",
        "  axes[0].plot(pos[:,0], pos[:,1], \n",
        "      c=cmap(angle[id]/360), alpha=0.5, linewidth=1)\n",
        "\n",
        "for id, pos in enumerate(hand_movement[:,:-10]):\n",
        "  axes[1].plot(pos[:,0], pos[:,1], \n",
        "      c=cmap(angle[id]/360), alpha=0.5, linewidth=1)\n",
        "\n",
        "#Plotting stuff\n",
        "axes[0].set_xlabel('x (cm)'), axes[0].set_ylabel('y (cm)')\n",
        "axes[0].set_title('Estimated hand movement')\n",
        "\n",
        "axes[1].set_xlabel('x (cm)')\n",
        "axes[1].set_title('True hand movement')\n",
        "\n",
        "plt.setp(axes,  xlim=(np.min(hand_movement[:,:,0]*1.1), np.max(hand_movement[:,:,0]*1.1)),\n",
        "                ylim=(np.min(hand_movement[:,:,1]*1.1), np.max(hand_movement[:,:,1]*1.1)))\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting, right? In fact, given sufficient data (not much more than this dataset), latent variables that generalize across recording sessions or even animals can be fitted. That is, only $U,M$ need to be refit to new trials or neurons, for which there are analytic solutions since they act only linearly on the latent variables.\n",
        "\n",
        "*  What does this suggest regarding the role of area A2 in kinematics of the hand?\n",
        "\n",
        "*  What stronger experiment might you do to probe the relationship between hand kinematics and area A2.\n",
        "\n",
        "*  How might you improve on this model? (e.g. with respect to the variable timing of movement you saw earlier)\n",
        "\n",
        "*  What might be a practical application of such a model?"
      ],
      "metadata": {
        "id": "VeLLqXMfrkhn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxe86vexfBjL"
      },
      "source": [
        "## Latent activity\n",
        "\n",
        "We now wish to understand *how* the model implements this trial-to-trial variability that seems to be correlated with behavior.\n",
        "\n",
        "To get a sense of what the latent activity looks like, we plot the first three latent variables for all trials, that is $\\mathbf{x}^{(i)}_j(t)$, $i\\in[364], j\\in [3]$. Furthermore, we color each curve according the the angle of the bump in that trial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4koo1IipTbg"
      },
      "outputs": [],
      "source": [
        "number_of_latents_plotted = 3\n",
        "\n",
        "fig, axes = plt.subplots(1, number_of_latents_plotted, \n",
        "              figsize=(1+3*number_of_latents_plotted,number_of_latents_plotted),\n",
        "              constrained_layout=True)\n",
        "\n",
        "ts = np.arange(100,500,10)\n",
        "\n",
        "for j in range(number_of_latents_plotted):\n",
        "  axes[j].set_title('$\\mathbf{x}^{(i)}_{'+str(j+1)+'}$')\n",
        "  for id, xs in enumerate(x):\n",
        "    axes[j].plot(ts, xs[:,j], c=cmap(angle[id]/360), alpha=0.4, linewidth=1)\n",
        "    axes[j].set_xlabel('time (ms)')\n",
        "\n",
        "fig.suptitle('Latent variables')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U1ufwCbfkki"
      },
      "source": [
        "*  What do you observe? (hint: colors)\n",
        "*  How might you check that the trial-to-trial variability for a given bump direction is lower than the bump direction-to-bump direction variability?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21KPr6bwlc74"
      },
      "source": [
        "##PCA\n",
        "\n",
        "However, looking at the latent variables one at a time is quite arbitrary. Instead, we can project them in a 3-dimensional space. A common choice for such a projection is the one that maximizes the variance explained of the data. Or equivalently, the one that minimizes the error when projecting back to the full space. That is $V\\in \\mathbb{R}^{m\\times 3}$ orthogonal such that,\n",
        "\n",
        "$$\n",
        "\\sum_i^K||\\mathbf{y}^{(i)}-V^{T}V\\mathbf{x}^{(i)}||^2\n",
        "$$\n",
        "\n",
        "is minimized. This is called [principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzfyrnwqmclV"
      },
      "outputs": [],
      "source": [
        "U, S, V = scipy.linalg.svd(x.reshape(-1,m), full_matrices=False)\n",
        "\n",
        "x_on_pc = x @ V.T[:,:3]\n",
        "\n",
        "fig = plt.figure(figsize=(4,4), constrained_layout=True)\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "for id, xs in enumerate(x_on_pc):\n",
        "    ax.plot(xs[:,0], xs[:,1], xs[:,2], c=cmap(angle[id]/360), alpha=0.4, linewidth=1.2)\n",
        "\n",
        "ax.set_xlabel('PC1'), ax.set_ylabel('PC2'), ax.set_zlabel('PC3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBuRk2MLsThH"
      },
      "source": [
        "*  Compared to looking at a single latent variable at a time, what do you observe here?\n",
        "\n",
        "*  Both $A$ derived earlier and the $V$ considered here are simply linear projections. How do their objective differ from:\n",
        "  *  a mathematical standpoint?\n",
        "  *  a neuroscientific standpoint?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial condition\n",
        "\n",
        "Trajectories of dynamical systems with no input are purely dependent on their initial condition. Therefore, if trajectories of the model seem bump direction-specific, we also expected the initial condition to be. To explore this, we plot $U$$-$which as a reminder is of shape $K \\times R$ where $K$ is the number of trials and $R$ the number of basis initial conditions$-$sorted by bump directions."
      ],
      "metadata": {
        "id": "1MQPRDr2WXPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_Us_plotted = 3\n",
        "\n",
        "U = np.array(params['U']) #Jax to np\n",
        "\n",
        "fig, axes = plt.subplots(number_of_Us_plotted, 1,\n",
        "              figsize=(4, 2*number_of_Us_plotted),\n",
        "              constrained_layout=True)\n",
        "\n",
        "sorted_id = np.argsort(angle)\n",
        "sorted_angle = angle[sorted_id]\n",
        "\n",
        "U_sorted = U[sorted_id]\n",
        "\n",
        "for i in range(number_of_Us_plotted):\n",
        "  for a in np.unique(angle):\n",
        "\n",
        "    axes[i].scatter(sorted_id[sorted_angle==a], U_sorted[sorted_angle==a,i],\n",
        "                    color=cmap(a/360), s=10)\n",
        "    \n",
        "    axes[i].set_ylabel('$U_'+str(i)+'$')\n",
        "      \n",
        "axes[0].set_title('Coefficient of basis initial conditions')\n",
        "axes[-1].set_xlabel('trial')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0barGzXJWW2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  What do you observe? \n",
        "*  How does this compare to the tuning of single neurons that you explored earlier?"
      ],
      "metadata": {
        "id": "eObIprB4bOcB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkTpjaptkVIt"
      },
      "source": [
        "## Analytical standpoint\n",
        "\n",
        "Finally, we may wish to get insight into the fitted model by analytically describing the dynamical system. While there are many approach one could take, here we take the simplest one: we look at the eigenspectrum of $W$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPEA-Stiqhr4"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(4,4), constrained_layout=True)\n",
        "ax = fig.add_subplot()\n",
        "\n",
        "L_W, _ = scipy.linalg.eig(W)\n",
        "ax.scatter(L_W.real, L_W.imag, label='pre-training')\n",
        "\n",
        "L_W, _ = scipy.linalg.eig(params['W'])\n",
        "ax.scatter(L_W.real, L_W.imag, label='post-training')\n",
        "\n",
        "ax.set_xlabel('Re'), ax.set_ylabel('Im')\n",
        "ax.legend()\n",
        "ax.axvline(0, color='black', linewidth=0.8)\n",
        "ax.axhline(0, color='black', linewidth=0.8)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQwCc1wVwCd7"
      },
      "source": [
        "*  Pick a few eigenvalues of interest and comment on their meaning. 1) From a mathematical standpoint, and 2) from the standpoint of the population of neurons the model is trying to capture."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "We started with a hypothesis that phenomenological models typically applied to recordings of the motor cortex could be repurposed to area A2. This was motivated by the findings of [1] that kinematics might be represented in the activity of area A2. As a first step, we explored the behavioral and neural data to get a better understanding of them and to decide whether our model of choice, PoissonLDS, was relevant at all. Then, we fitted that model to neural data and observed that it captured variability in the data that was relevant to behavior. Indeed, hand movements could be relatively well linearly decoded from the trajectories of the model, and the initial condition of the model seemed tuned to angles. Finally, we hinted at the fact that the model, unlike the data, could be *mathematically* analyzed. These steps, starting at our original hypothesis, might be a good glimpse into research involving neural recordings, would you decide to ongo down that path. "
      ],
      "metadata": {
        "id": "6SJX8mYWY6qr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP7VEBVqy-GN"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Chowdhury, R. H., Glaser, J. I., & Miller, L. E. (2020). Area 2 of primary somatosensory cortex encodes kinematics of the whole arm. Elife, 9.\n",
        "\n",
        "[2] Macke, J. H., Buesing, L., Cunningham, J. P., Yu, B. M., Shenoy, K. V., & Sahani, M. (2011). Empirical models of spiking in neural populations. Advances in neural information processing systems, 24.\n",
        "\n",
        "[3] Seely, J. S., Kaufman, M. T., Ryu, S. I., Shenoy, K. V., Cunningham, J. P., & Churchland, M. M. (2016). Tensor analysis reveals distinct population structure that parallels the different computational roles of areas M1 and V1. PLoS computational biology, 12(11), e1005164."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-khOJ_4KHdhn"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}