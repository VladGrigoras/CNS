{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VladGrigoras/CNS/blob/main/CNS_Lab4_NeuralCoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZl1Ri63YL97"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "sns.set()\n",
        "\n",
        "# global defaults for plots - optional\n",
        "sns.set_theme(style=\"ticks\",\n",
        "              palette=\"colorblind\",\n",
        "              font_scale=1.7,\n",
        "              rc={\n",
        "              \"axes.spines.right\": False,\n",
        "              \"axes.spines.top\": False,\n",
        "          },\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Coding\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AS7BurNrTM2g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Grr4MkYL95"
      },
      "source": [
        "## Introduction \n",
        "\n",
        "In this lab we focus on population coding, decoding and Fisher information. We investigate how well a stimulus can be decoded from the response of one (or multiple) neurons, how the Fisher information can be used to quantify decoder performance, and when Fisher information fails to capture the behaviour of simple decoders."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maximum Likelihood for Poisson Neurons\n",
        "\n",
        "We first consider how the response of a single neuron can be used to decode a stimulus that the neuron is tuned to, and how decoding from multiple neurons can improve decoding performance.\n",
        "\n",
        "Consider a neuron with response $p(n|s) = Poiss(f(s)T) = (f(s)T)^ne^{-f(s)T}/n!$ where $f(s) = r_{max}e^{-(s-s_{pref})^2/(2 \\sigma_f^2)}$. Set $r_{max} = 20 Hz$, $T=1 s$, and $\\sigma_f = 20^\\circ$, with $s\\in\\left[-180^\\circ, 180^\\circ\\right)$\n",
        "\n",
        "* Plot the likelihood function $p(n|s)$ for a neuron with $s_{pref}=0^\\circ$ for measured responses $n\\in\\left\\{0, 1, ..., 40\\right\\}$. What do you observe? How do you interpret this?\n",
        "* Derive the stimulus values $s^*$ for which the (log) likelihood is extremised for a given $n$. How does this compare to your numerical results? What are the values $\\hat{s}_{ML}$ at which the likelihood is maximised?\n",
        "* An alternative way to decode is maximum a posteriori, which introduces a prior over the stimulus $p(s)$ and defines $p(s|n) = p(n|s) p(s)/p(n)$. How might the introduction of a prior influence decoded stimulus in this case?\n",
        "* Consider two independent neurons with overlapping tuning curves with $s_{pref}^{(1)} = -5^\\circ$ and $s_{pref}^{(2)} = 5^\\circ$. How would you expect the likelihood function to change when decoding from both neurons simultaneously? Plot the likelihood $p(\\mathbf{n}|s) = p(n_1|s) p(n_2|s)$ to verify your prediction. Try varying $n_1$ and $n_2$ to see how the likelihood function varies.\n",
        "\n"
      ],
      "metadata": {
        "id": "uCyAPlzAbU1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution\n",
        "\n"
      ],
      "metadata": {
        "id": "uwNkPu4OUj_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmax = 20;\n",
        "sigmaf = 20;\n",
        "spref = 0;\n",
        "s=np.arange(-180, 180, 1)\n",
        "T = 1;\n",
        "\n",
        "# Your solution here:"
      ],
      "metadata": {
        "id": "LYyAcmyZUqDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimators and Fisher Information\n",
        "\n",
        "Fisher information quantifies the performance of an optimal unbiased estimator of the stimulus $s$ from the response $n$. \n",
        "\n",
        "* Simulate trials of the single neuron discussed above in response to the stimulus $s=0$ and decode the stimulus on each trial (use np.random.poisson to generate the response on each trial). As we investigated above, you will find that in many cases two stimuli are equally likely - to handle such cases, try two different decoding strategies, one in which you pick one of the two most likely stimuli at random with probability $p=0.5$, and another in which you always pick the stimulus to the left of the tuning curve. Compute the bias $b(s) = \\langle \\hat{s} - s \\rangle$, variance $var(s) = \\langle (\\hat{s} - \\langle \\hat{s}\\rangle )^2 \\rangle$, and mean squared error  $MSE(s) = \\langle (\\hat{s} - s )^2 \\rangle$ for each decoder (averages are with respect to $p(n|s)$ for fixed $s$). How do these quantities differ between the two decoders? [Hint: performing maximum likelihood numerically is prone to numerical precision errors - it is much simpler to use the analytical results derived in the previous section to find the maxima $\\hat{s}_{ML}(n)$].\n",
        "* The Fisher information is defined as $I_F(s) = -\\langle \\frac{\\partial^2 \\log p(n|s)}{\\partial s^2}\\rangle$. The Cramer-Rao bound states that $var(s) \\ge 1/I_F(s)$ for any *unbiased* estimator. Derive an expression for the Fisher information for the one and two neuron cases considered above. \n",
        "* Simulate and plot the bias, variance, and MSE of the two decoders as a function of the presented stimulus $s$, and compare to a plot of the Fisher information. Is there any correspondence between the maxima and minima of the Fisher information and the bias, variance, and MSE of the two decoders?\n",
        "* For the single-neuron case, the Fisher information is zero for stimuli presented at the tuning curve centre ($I_F(0) = 0$), which by the Cramer-Rao bound suggests that the variance of an unbiased estimator should be infinite. Why do we not see infinite variance for either of our decoders?\n",
        "* The Cramer Rao bound can be generalised to the case of a biased estimator as $MSE(s) \\ge \\frac{[1+b'(s)]^2}{I_F(s)} + b^2(s)$. Compute this lower bound for each decoder and compare against their respective MSEs. Do your decoders satisfy this lower bound? [Hint: You will need to compute $b'(s)$ numerically using np.gradient]\n",
        "* Consider a large population of independent neurons with tuning curves $f_i(s) = f(s-s_i)$ which uniformly tile the stimulus space $\\left[-180,180\\right)$. What do you think would happen to the bias, variance, and MSE of the maximum likelihood decoder of this population? Would the basic form of the Cramer-Rao lower bound hold in this case? Derive an expression for the Fisher information of such a population. What happens to the Fisher information as $N\\rightarrow\\infty$, and what would happen to the decoding error?\n"
      ],
      "metadata": {
        "id": "36h89n7m-oM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ntrials = 100000\n",
        "\n",
        "sdec1 = np.zeros([Ntrials,1]) # stimulus decoded using decoder 1 on each trial\n",
        "sdec2 = np.zeros([Ntrials,1]) # stimulus decoded using decoder 2 on each trial\n",
        "\n",
        "# Your solution here:"
      ],
      "metadata": {
        "id": "MGoCE1-9sCaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ntrials = 10000 # we can use a smaller number of trials to reduce computation time since we have to loop over stimulus values\n",
        "\n",
        "# Your solution here:"
      ],
      "metadata": {
        "id": "8oHedmx0HqMp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}